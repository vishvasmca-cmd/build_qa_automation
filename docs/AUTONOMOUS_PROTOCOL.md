# Autonomous Protocol: Operating Without Manual Intervention

The core design of this framework is to be **Self-Correcting**, meaning it should solve its own problems through a data-driven learning loop. Here is how you can ensure the pipeline passes without manually editing code or providing hints.

## 1. The "Memory" Mechanism
Everything the agent learns is stored in two places:
1.  **`knowledge/failures.json`**: A history of every failure, what was attempted, and why it failed.
2.  **`knowledge/sites/{domain}/rules.md`**: Site-specific instructions (Negative & Positive) generated by the Feedback Agent.

## 2. The Learning Cycle (How it succeeds alone)
If a test fails initially, the internal logic follows this "Protocol":
1.  **Execution Failure**: `pytest` returns an error (e.g., `TimeoutError`).
2.  **Feedback Distillation**: The **Feedback Agent** (LLM) analyzes the execution log + screenshots and updates `rules.md` (e.g., "⚠️ PROHIBITED: Don't click X until Y is visible").
3.  **Autonomous Re-gen**: The **Orchestrator** triggers a second attempt. The **Refiner** reads the *updated* `rules.md` and generates a *new* version of `test_main.py` that avoids the previous mistake.

## 3. Your Role as "Supervisor"
Instead of helping with code, you can help by **resetting the environment** if it gets stuck:
- **Clean Start**: If you want a fresh run, delete the `.checkpoint.json` in the project folder.
- **Deep Clean**: If the agent has learned a "wrong" rule, edit the `rules.md` to remove it.

## 4. Observing the Success
You can watch the transition in `outputs/master_status.json`. If you see an agent status change from `error` to `healing` to `working`, the system is autonomously repairing itself.
